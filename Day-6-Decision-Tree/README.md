# ğŸŒ³ Day 6 â€” Decision Trees & Random Forests

Welcome to Day 6!  This module covers one of the most intuitive and powerful machine learning algorithms: **Decision Trees** and their ensemble variant **Random Forests**. 

---

## ğŸ“š Learning Objectives

By the end of this module, you will:

- âœ… Understand decision tree structure (root, nodes, leaves, branches)
- âœ… Calculate **entropy** and **information gain** manually
- âœ… Build a decision tree classifier from scratch
- âœ… Visualize decision trees and decision boundaries
- âœ… Apply tree pruning to prevent overfitting
- âœ… Use Random Forests for ensemble learning
- âœ… Interpret feature importance from trees

---

## ğŸŒ² Key Concepts

### What is a Decision Tree? 

A **decision tree** is a flowchart-like structure where:
- Each **internal node** represents a test on a feature (e.g., "Is age > 30?")
- Each **branch** represents the outcome of that test
- Each **leaf node** represents a class label (classification) or value (regression)

**Example:**

```
                [Age > 30?]
             /               \
          Yes                 No
         /                      \
  [Income > 50k?]            [Student?]
   /        \                 /      \
 Yes        No               Yes      No
 /           \               /          \
Approve    Reject         Reject       Approve
```

### Why Decision Trees? 

âœ… **Easy to understand and interpret** â€” Human-readable rules  
âœ… **No feature scaling needed** â€” Works with raw data  
âœ… **Handles non-linear relationships** â€” Captures complex patterns  
âœ… **Feature importance** â€” Shows which features matter most  
âŒ **Prone to overfitting** â€” Can memorize training data  
âŒ **Unstable** â€” Small data changes can produce very different trees  

---

## ğŸ§® Core Concepts Explained

### 1.  Entropy

**Entropy** measures the impurity or disorder in a dataset. 

**Formula:**
```
Entropy(S) = -Î£ p_i * logâ‚‚(p_i)
where `p_i` is the proportion of class `i` in set `S`.
```


**Interpretation:**
- Entropy = 0 â†’ Pure set (all samples belong to one class)
- Entropy = 1 â†’ Maximum impurity (equal distribution of classes)

**Example:**
- Dataset: [Yes, Yes, No, No] â†’ 50% Yes, 50% No
- Entropy = -0.5 * logâ‚‚(0. 5) - 0.5 * logâ‚‚(0.5) = 1. 0 (maximum impurity)

### 2. Information Gain

**Information Gain** measures how much entropy is reduced after a split.

**Formula:**
```
IG(S, A) = Entropy(S) - Î£ (|S_v| / |S|) * Entropy(S_v)

where:
- `S` = parent dataset
- `A` = attribute (feature) to split on
- `S_v` = subset of S where attribute A has value v

```

### 3.  Gini Impurity (Alternative to Entropy)

**Gini Impurity** is another measure of impurity used by sklearn's default decision trees.

**Formula:**
```
Gini(S) = 1 - Î£ p_iÂ²

```

**Comparison:**
- Entropy and Gini give similar results
- Gini is computationally faster (no logarithm)
- Entropy penalizes impurity slightly more

### 4. Tree Pruning

**Pruning** reduces tree complexity to prevent overfitting. 

**Pre-pruning (Early Stopping):**
- Set `max_depth` (limit tree depth)
- Set `min_samples_split` (minimum samples to split a node)
- Set `min_samples_leaf` (minimum samples in a leaf)

**Post-pruning (Cost-Complexity Pruning):**
- Grow a full tree, then remove branches that provide little value
- sklearn uses `ccp_alpha` parameter for cost-complexity pruning

### 5. Random Forests

**Random Forest** is an ensemble of many decision trees.

**How it works:**
1. Create multiple decision trees (e.g., 100 trees)
2. Each tree is trained on a **random subset** of data (bootstrap sampling)
3. Each split considers only a **random subset** of features
4.  Final prediction = **majority vote** (classification) or **average** (regression)

**Why Random Forests are better:**
- âœ… Reduces overfitting (averaging reduces variance)
- âœ… More robust and stable
- âœ… Better generalization
- âœ… Can estimate feature importance across all trees

---

## ğŸ“ Files in This Module
```bash
|             File                |                                  Description                               |
|---------------------------------|----------------------------------------------------------------------------|
| `README.md`                     |                  This file â€” concepts and learning guide                   |
| `decision_tree_basics.py`       |           Introduction to sklearn DecisionTreeClassifier with examples     |
| `entropy_information_gain.py`   |            Manual calculation of entropy and information gain              |
| `decision_tree_from_scratch.py` |              Complete decision tree implementation from scratch            |
| `tree_visualization.py`         |                      Visualize trees and decision boundaries               |
| `tree_pruning_example.py`       |             Demonstrate pre-pruning and post-pruning techniques            |
| `random_forest_ensemble.py`     |              Compare single tree vs Random Forest performance              |
| `feature_importance_analysis.py`|                  Extract and interpret feature importances                 |
| `common_mistakes.txt`           |                     Common pitfalls and best practices                     |

---
```
## ğŸš€ How to Run Examples

```bash
# Navigate to Day 6 folder
cd Day-6-Decision-Trees-Random-Forests

# Run any example
python decision_tree_basics.py
python entropy_information_gain.py
python decision_tree_from_scratch.py
python tree_visualization.py
python tree_pruning_example.py
python random_forest_ensemble.py
python feature_importance_analysis.py

```

## ğŸ¯ Learning Path
Recommended order:

1. Start with README. md (you are here!) â€” Understand core concepts
2. decision_tree_basics.py â€” See sklearn trees in action
3. entropy_information_gain.py â€” Master the math behind splits
4. decision_tree_from_scratch.py â€” Understand the algorithm deeply
5. tree_visualization. py â€” Visualize what trees actually learn
6. tree_pruning_example.py â€” Learn to control overfitting
7. random_forest_ensemble.py â€” See the power of ensembles
8. feature_importance_analysis.py â€” Interpret your models
9. common_mistakes. txt â€” Avoid common pitfalls


## ğŸ§  Key Takeaways
-Decision trees split data recursively to maximize information gain (or minimize Gini impurity)
Entropy measures disorder; information gain measures entropy reduction
Trees are prone to overfitting â†’ use pruning or Random Forests
Random Forests improve performance by averaging many trees
Feature importance helps interpret which features drive predictions
No feature scaling needed for tree-based models

## ğŸ”— Connections to Other Days
Day 1 (Feature Scaling): Trees don't need scaling, but other models do
Day 3 (Linear Regression): Trees capture non-linear patterns better
Day 4 (Logistic Regression): Trees create non-linear decision boundaries
Day 5 (Preprocessing): Trees handle missing values and categorical features naturally (but encoding still helps)



Happy Learning! ğŸŒ³ğŸ“


