Common Mistakes with Decision Trees & Random Forests
=====================================================

This file lists frequent pitfalls when using decision trees and Random Forests, 
with explanations and how to avoid them.

---

1. NOT LIMITING TREE DEPTH (Overfitting)
-----------------------------------------

MISTAKE:
    tree = DecisionTreeClassifier()
    tree.fit(X_train, y_train)

WHY IT'S WRONG:
    Without constraints, the tree grows until all leaves are pure. 
    This leads to severe overfitting — the tree memorizes training data.

CORRECT APPROACH:
    Use pre-pruning hyperparameters:
    - max_depth: Limit tree depth (e.g., 5-10 for most datasets)
    - min_samples_split: Minimum samples to split a node (e.g., 10-50)
    - min_samples_leaf: Minimum samples in a leaf (e.g., 5-20)
    
    tree = DecisionTreeClassifier(max_depth=7, min_samples_split=20)
    tree.fit(X_train, y_train)

ALTERNATIVE:
    Use cost-complexity pruning:
    tree = DecisionTreeClassifier(ccp_alpha=0.01)

---

2. NOT USING RANDOM FORESTS WHEN APPROPRIATE
---------------------------------------------

MISTAKE:
    Using a single decision tree for production models.

WHY IT'S PROBLEMATIC:
    Single trees are:
    - Unstable (small data changes cause big tree changes)
    - High variance (prone to overfitting)
    - Less accurate than ensembles

CORRECT APPROACH:
    Use Random Forests for better performance:
    forest = RandomForestClassifier(n_estimators=100, max_depth=10)
    forest.fit(X_train, y_train)

WHEN TO USE SINGLE TREES:
    - When interpretability is critical (single tree is easier to explain)
    - For quick prototyping or debugging
    - When computational resources are extremely limited

---

3.  SCALING FEATURES BEFORE TREES (Unnecessary Work)
----------------------------------------------------

MISTAKE:
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    tree = DecisionTreeClassifier()
    tree.fit(X_scaled, y)

WHY IT'S UNNECESSARY:
    Decision trees are invariant to monotonic transformations.
    Scaling does NOT affect tree performance (splits stay the same).

CORRECT APPROACH:
    Skip scaling for tree-based models:
    tree = DecisionTreeClassifier()
    tree.fit(X, y)

WHEN SCALING MATTERS:
    - Linear models (regression, logistic regression, SVM)
    - Distance-based models (KNN, K-means)
    - Neural networks

---

4. USING DEFAULT HYPERPARAMETERS IN PRODUCTION
-----------------------------------------------

MISTAKE:
    forest = RandomForestClassifier()
    forest.fit(X_train, y_train)

WHY IT'S PROBLEMATIC:
    Default hyperparameters may not be optimal for your dataset.
    You might get better performance with tuning.

CORRECT APPROACH:
    Use GridSearchCV or RandomizedSearchCV:
    
    from sklearn.model_selection import GridSearchCV
    
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 10, 20],
        'min_samples_leaf': [1, 5, 10]
    }
    
    grid_search = GridSearchCV(
        RandomForestClassifier(random_state=42),
        param_grid,
        cv=5,
        scoring='accuracy'
    )
    grid_search.fit(X_train, y_train)
    best_forest = grid_search.best_estimator_

---

5. IGNORING CLASS IMBALANCE
----------------------------

MISTAKE:
    Training on imbalanced data without adjustments. 
    Example: 95% class 0, 5% class 1

WHY IT'S PROBLEMATIC:
    Trees will bias toward the majority class.
    Minority class predictions will be poor.

CORRECT APPROACH:
    Option 1: Use class_weight parameter
    tree = DecisionTreeClassifier(class_weight='balanced')
    
    Option 2: Use RandomForest with balanced weights
    forest = RandomForestClassifier(class_weight='balanced')
    
    Option 3: Oversample minority class or undersample majority class
    from imblearn.over_sampling import SMOTE
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

---

6. USING TOO FEW TREES IN RANDOM FOREST
----------------------------------------

MISTAKE:
    forest = RandomForestClassifier(n_estimators=10)

WHY IT'S PROBLEMATIC:
    Too few trees → high variance, unstable predictions. 

CORRECT APPROACH:
    Use at least 100 trees (more is better, but diminishing returns):
    forest = RandomForestClassifier(n_estimators=100)
    
    For critical applications:
    forest = RandomForestClassifier(n_estimators=500)

TRADE-OFF:
    More trees = better performance but slower training/prediction.

---

7.  MISINTERPRETING FEATURE IMPORTANCE
--------------------------------------

MISTAKE:
    Assuming feature importance = causal effect.

WHY IT'S WRONG:
    Feature importance shows correlation, NOT causation.
    - Correlated features share importance
    - Importance can be biased by feature cardinality

CORRECT INTERPRETATION:
    Feature importance tells you:
    - Which features the model uses most
    - Which features contribute to prediction accuracy
    
    It does NOT tell you:
    - Which features cause the outcome
    - Which features are truly important in the real world

RECOMMENDATION:
    Use feature importance for:
    - Feature selection
    - Understanding model behavior
    - Generating hypotheses (not conclusions)

---

8. NOT HANDLING MISSING VALUES PROPERLY
----------------------------------------

MISTAKE:
    Dropping all rows with missing values before training:
    df_clean = df.dropna()

WHY IT'S PROBLEMATIC:
    You lose data and potentially introduce bias. 

CORRECT APPROACH:
    Sklearn trees (since v1.3) can handle missing values natively:
    tree = DecisionTreeClassifier()
    tree.fit(X_with_nans, y)
    
    Alternatively, impute explicitly:
    from sklearn.impute import SimpleImputer
    imputer = SimpleImputer(strategy='median')
    X_imputed = imputer.fit_transform(X)

---

9. USING TREES FOR EXTRAPOLATION
---------------------------------

MISTAKE:
    Using decision trees to predict outside the training data range.

WHY IT'S WRONG:
    Trees cannot extrapolate — they can only predict values seen in training leaves. 
    
    Example:
    - Training target range: [10, 100]
    - Tree can NEVER predict < 10 or > 100

CORRECT APPROACH:
    If extrapolation is needed, use linear models or explicitly handle edge cases. 

---

10. FORGETTING TO SET random_state
-----------------------------------

MISTAKE:
    tree = DecisionTreeClassifier()
    forest = RandomForestClassifier()

WHY IT'S PROBLEMATIC:
    Results are not reproducible.
    Different runs give different trees and scores.

CORRECT APPROACH:
    Always set random_state for reproducibility:
    tree = DecisionTreeClassifier(random_state=42)
    forest = RandomForestClassifier(random_state=42)

---

11. NOT VALIDATING ON SEPARATE TEST SET
----------------------------------------

MISTAKE:
    Evaluating only on training data or using cross-validation without a final test set.

WHY IT'S WRONG:
    Training accuracy is always optimistic (especially for trees).
    Cross-validation helps, but final test set is gold standard.

CORRECT APPROACH:
    1. Split: train / validation / test
    2.  Tune hyperparameters on validation set (or use CV on train set)
    3.  Evaluate final model on test set (only once!)

---

12. USING GINI VS ENTROPY WITHOUT UNDERSTANDING
------------------------------------------------

MISTAKE:
    Blindly choosing criterion='entropy' or criterion='gini'. 

UNDERSTANDING:
    - Gini impurity (default): Faster to compute, slightly favors larger partitions
    - Entropy (information gain): Slightly slower, more balanced splits
    - In practice: Both give similar results

RECOMMENDATION:
    Stick with default (Gini) unless you have a specific reason to use Entropy.

---

13. ASSUMING TREES WORK WELL ON ALL DATASETS
---------------------------------------------

MISTAKE:
    Using trees for datasets where other models are better. 

WHEN TREES STRUGGLE:
    - Very high-dimensional sparse data (e.g., text with 10,000+ features)
    - Datasets with strong linear relationships (linear regression is better)
    - Small datasets (< 100 samples) — trees overfit easily

WHEN TREES EXCEL:
    - Mixed feature types (numeric + categorical)
    - Non-linear relationships
    - Datasets with interactions between features
    - Medium to large datasets (1000+ samples)

---

14. NOT CONSIDERING TREE DEPTH IN PRODUCTION
---------------------------------------------

MISTAKE:
    Deploying very deep trees without considering inference time.

WHY IT MATTERS:
    Deep trees = more nodes to traverse = slower predictions. 

CORRECT APPROACH:
    Balance accuracy vs speed:
    - For real-time systems: Limit depth (max_depth=10)
    - For batch processing: Depth matters less

OPTIMIZATION:
    Use max_leaf_nodes instead of max_depth for finer control:
    tree = DecisionTreeClassifier(max_leaf_nodes=50)

---

15. IGNORING OUT-OF-BAG (OOB) SCORE IN RANDOM FORESTS
------------------------------------------------------

MISTAKE:
    Not using OOB score as a free validation metric. 

WHAT IS OOB:
    Each tree in a Random Forest uses a bootstrap sample (~63% of data).
    The remaining ~37% (OOB samples) can be used for validation. 

CORRECT APPROACH:
    Enable OOB scoring:
    forest = RandomForestClassifier(n_estimators=100, oob_score=True)
    forest.fit(X, y)
    print(f"OOB Score: {forest.oob_score_}")

BENEFIT:
    OOB score is a free estimate of generalization without a separate validation set.

---

SUMMARY CHECKLIST
-----------------

Before deploying a tree-based model, ask:

- ✅ Have I limited tree depth (max_depth, min_samples_split)? 
- ✅ Am I using Random Forest instead of a single tree?
- ✅ Have I tuned hyperparameters (not using defaults)?
- ✅ Have I handled class imbalance (if applicable)?
- ✅ Have I used enough trees (n_estimators >= 100)?
- ✅ Do I understand feature importances are correlations, not causation?
- ✅ Have I handled missing values appropriately?
- ✅ Have I set random_state for reproducibility? 
- ✅ Have I validated on a separate test set?
- ✅ Is my dataset suitable for tree-based models? 

---
End of common_mistakes.txt
